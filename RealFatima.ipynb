{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RealFatima",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMsYsQCprfo9m0PQWeNNy1U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subha-v/FatimaFellowshipProjectPublic/blob/main/RealFatima.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will import the necessary packages for the project."
      ],
      "metadata": {
        "id": "dwuIHhlF6rx5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UMwlXIh3It3y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will import the datasets into the project."
      ],
      "metadata": {
        "id": "HYvPY4uV6xHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fakenews = pd.read_csv('/content/Fake.csv')\n",
        "truenews = pd.read_csv('/content/True.csv')"
      ],
      "metadata": {
        "id": "YzExRjggvBwB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're looking at the datasets to get a general idea of how they are formatted"
      ],
      "metadata": {
        "id": "3bnqW1c5626t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fakenews.head()"
      ],
      "metadata": {
        "id": "aKJmRNxlwNYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are combining both the datasets into one dataset and also adding a column regarding whether the news is true or fake"
      ],
      "metadata": {
        "id": "Fl4Ug38X6917"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fakenews['temp']= 0\n",
        "truenews['temp']= 1\n",
        "\n",
        "combined_dataset = pd.DataFrame()\n",
        "combined_dataset = truenews.append(fakenews)"
      ],
      "metadata": {
        "id": "Bj_lD8DsxS-7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taking a look at the new dataset"
      ],
      "metadata": {
        "id": "lxzg_8Ch7O4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dataset"
      ],
      "metadata": {
        "id": "1Gr5e4Eexe0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't need the columns of date and subject for right now"
      ],
      "metadata": {
        "id": "hsEaOZ2V7wBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column = ['date','subject']\n",
        "combined_dataset = combined_dataset.drop(columns=column)"
      ],
      "metadata": {
        "id": "XVIIuTfvxndT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can combine the title of the article with the body text and drop the title as well"
      ],
      "metadata": {
        "id": "DsXxXOdI7yYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dataset[\"text\"] = combined_dataset[\"text\"]+\" \"+ combined_dataset[\"title\"]"
      ],
      "metadata": {
        "id": "brmm6ETFyK6p"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dataset = combined_dataset.drop(columns=\"title\")"
      ],
      "metadata": {
        "id": "7lM7g0uaAlnh"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we will process the data using NLTK\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlMK6QrzyO4k",
        "outputId": "ffad4d3d-a401-4132-9efc-2582ec336e8e"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "st = SnowballStemmer('english')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "0Iv8htjCYZBL"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text):  \n",
        "    return re.sub(r'[^\\w\\s]','', str(text))\n",
        "                  \n",
        "                  \n",
        "def remove_stopwords(text):  #Removing stopwords(eg. this, that, am, be etc)\n",
        "    stop = stopwords.words(\"english\")\n",
        "    final_text = []\n",
        "    for i in str(text).split():\n",
        "        if i.strip() not in stop:\n",
        "            final_text.append(i.strip())\n",
        "    return \" \".join(final_text)\n",
        "                  \n",
        "def tokenize(text):  \n",
        "    tokens = re.split('\\W+',text) #W+ means that either a word character (A-Z) or a dash(-) can go there.\n",
        "    return tokens\n",
        "\n",
        "                          \n",
        "def stemming(text):  \n",
        "    porter_stemmer = PorterStemmer()\n",
        "    return porter_stemmer.stem(str(text))\n",
        "                  \n",
        "def lemmatization(text):  #Applying Lemaatization \n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    return wordnet_lemmatizer.lemmatize(str(text))"
      ],
      "metadata": {
        "id": "ObLJm9GQA2F6"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  >>> import nltk\n",
        "  >>> nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIEld9lYBTGo",
        "outputId": "718fee7e-fcc0-415e-863f-de7056d521bf"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = remove_punctuation(text)\n",
        "    text = remove_stopwords(text)\n",
        "    text = stemming(text)\n",
        "    return lemmatization(text)\n",
        "\n",
        "combined_dataset['text'] = combined_dataset['text'].apply(lambda x: clean_text(x))"
      ],
      "metadata": {
        "id": "vpxDfbEMBF5J"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_text = combined_dataset['text']"
      ],
      "metadata": {
        "id": "hz5hRwgubRsZ"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "all_text = cv.fit_transform(all_text)"
      ],
      "metadata": {
        "id": "zNTIVZeal41K"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "truth_or_false = combined_dataset['temp']"
      ],
      "metadata": {
        "id": "LAVd-ma6bkCd"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import sequence, text\n",
        "from sklearn.model_selection import train_test_split\n",
        "xtrain, xvalid, ytrain, yvalid = train_test_split(all_text, truth_or_false, \n",
        "                                                  random_state=42, \n",
        "                                                  test_size=0.15, shuffle=True)\n"
      ],
      "metadata": {
        "id": "Gz5iUzz0aPva"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "LR = LogisticRegression()\n",
        "LR.fit(xtrain,ytrain)\n",
        "predictions2 = LR.predict(xvalid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ook9ob8_DWMv",
        "outputId": "9410eac7-fcfc-4b6f-80d5-8d0ef7a2917a"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "print(confusion_matrix(yvalid,predictions2))\n",
        "print('\\n')\n",
        "print(classification_report(yvalid,predictions2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ct1ED9sDtvg",
        "outputId": "f9b86770-aa22-48fe-adf2-2c3203144ff3"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3529    0]\n",
            " [   0 3206]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      3529\n",
            "           1       1.00      1.00      1.00      3206\n",
            "\n",
            "    accuracy                           1.00      6735\n",
            "   macro avg       1.00      1.00      1.00      6735\n",
            "weighted avg       1.00      1.00      1.00      6735\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A Naive Bayes Classifier determines the probability that an example belongs to some class, calculating the probability that an event will occur given that some input event has occurred."
      ],
      "metadata": {
        "id": "Lr1FoWahcMsW"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "metadata": {
        "id": "0yoX4gMqcOaT"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb = MultinomialNB()"
      ],
      "metadata": {
        "id": "FRPpW9tBloC8"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb.fit(xtrain, ytrain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvc0-xCdlqBh",
        "outputId": "41de9d5a-96e5-4d87-fb0e-9a2dad534382"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = nb.predict(xvalid)"
      ],
      "metadata": {
        "id": "dZyxMQPomHRA"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "print(confusion_matrix(yvalid,predictions))\n",
        "print('\\n')\n",
        "print(classification_report(yvalid,predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwuNlzr1mR66",
        "outputId": "2c5f5ea7-dbca-488f-8117-272f49bf55db"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3393  133]\n",
            " [ 105 3104]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.96      0.97      3526\n",
            "           1       0.96      0.97      0.96      3209\n",
            "\n",
            "    accuracy                           0.96      6735\n",
            "   macro avg       0.96      0.96      0.96      6735\n",
            "weighted avg       0.96      0.96      0.96      6735\n",
            "\n"
          ]
        }
      ]
    }
  ]
}